{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improving_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talitmr/text_preprocess_study/blob/main/improving_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAI8gIO4RBBC"
      },
      "source": [
        "# IS 784 Assignment1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQNN8CFoRBBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9adaed3a-991a-4c30-8c05-83f140bca4ae"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__) #version of the pytorh\n",
        "import torch.nn.functional as F\n",
        "import torchtext.legacy as torchtext\n",
        "import random\n",
        "\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import pandas as pd\n",
        "import html\n",
        "#import contractions\n",
        "#from langdetect import detect\n",
        "from nltk import download\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.data import load\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "import re\n",
        "import nltk.data\n",
        "from nltk import pos_tag_sents\n",
        "# If you have Anaconda, you can install emoji using \n",
        "# \"conda install -c conda-forge emoji\" command. You can download autocorrect using pip\n",
        "# and \"target\" parameter: \"pip install autocorrect --target=<directory>\"\n",
        "#from autocorrect import Speller\n",
        "#from emoji import get_emoji_regexp\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "#import Levenshtein\n",
        "from snowballstemmer import TurkishStemmer\n",
        "#from zeyrek import MorphAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRW0mcRQn_PK"
      },
      "source": [
        "## Finding Max Length \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9juFZANBZEqX"
      },
      "source": [
        "First thing we need to do is finding the max length of the text in the dataset because our input size is constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz3mTFB3OOAo"
      },
      "source": [
        "## In this cell create \"tokenize_fn\" that takes text and returns tokens\n",
        "\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def tokenize_fn(text):\n",
        "\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "  return tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Ni6oXTich-"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize= tokenize_fn, batch_first=True) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PEQVuI_m3uV"
      },
      "source": [
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac25bdxVXecP",
        "outputId": "8de1ed4e-0717-4d85-b714-ea2fe26cca20"
      },
      "source": [
        "max_size=0  ## this part of the code find maximum length of the network\n",
        "count=0\n",
        "sum= 0\n",
        "for i in  range(len(train_data)):\n",
        "  if max_size < len(train_data[i].text):\n",
        "    max_size =len(train_data[i].text)\n",
        "    print(max_size)\n",
        "  count +=1\n",
        "  sum +=len(train_data[0].text)\n",
        "print(\"avarage: \", sum/count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "559\n",
            "623\n",
            "1051\n",
            "1055\n",
            "1107\n",
            "1428\n",
            "1666\n",
            "1776\n",
            "1866\n",
            "1873\n",
            "2525\n",
            "avarage:  559.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7I6kZ7oHKX"
      },
      "source": [
        "## Training Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy6YWYP4oN58"
      },
      "source": [
        "We just have found the max length corpus. Now let's create our train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_wJl9cwmuR0",
        "outputId": "86260d88-07b9-4728-badb-952d30758f27"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize=tokenize_fn, batch_first=True,fix_length=max_size ) # preprocessing parameters can be used to add aditional  preprocessing steps\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL) \n",
        "print(\"train length is: \",len(train_data))\n",
        "print(\"test length is: \",len(test_data))\n",
        "print(vars(train_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length is:  25000\n",
            "test length is:  25000\n",
            "{'text': ['I', 'f', 'you', 'thought', 'Sam', 'Mendes', 'first', 'film', 'the', 'much', 'heralded', 'American', 'BEAUTY', 'was', 'a', 'movie', 'with', 'style', 'to', 'spare', 'wait', 'until', 'you', 'see', 'his', 'highly', 'anticipated', 'second', 'effort', 'the', 'unrelentingly', 'grim', '30', 's', 'gangster', 'melodrama', 'ROAD', 'TO', 'PERDITION', 'Some', 'critics', 'have', 'hailed', 'this', 'new', 'movie', 'as', 'a', 'worthy', 'successor', 'to', 'THE', 'GODFATHER', 'a', 'rash', 'judgment', 'made', 'by', 'several', 'reviewers', 'taken', 'with', 'Mr', 'Mendes', 'extraordinary', 'technical', 'prowess', 'If', 'the', 'mechanics', 'of', 'movie', 'making', 'are', 'what', 'make', 'a', 'picture', 'great', 'then', 'yes', 'ROAD', 'TO', 'PERDITION', 'is', 'a', 'distant', 'cousin', 'to', 'THE', 'GODFATHER', 'in', 'terms', 'of', 'what', 'it', 'achieves', 'in', 'cinematography', 'editing', 'music', 'scoring', 'and', 'sound', 'What', 'it', 'doesn', 't', 'have', 'is', 'a', 'resonance', 'that', 'all', 'great', 'stories', 'and', 'some', 'very', 'rare', 'movies', 'have', 'that', 'stay', 'with', 'the', 'viewer', 'long', 'after', 'the', 'experience', 'of', 'reading', 'or', 'seeing', 'it', 'is', 'over', 'As', 'with', 'American', 'BEAUTY', 'there', 'is', 'a', 'cold', 'distancing', 'feel', 'to', 'this', 'movie', 'despite', 'some', 'very', 'tense', 'scenes', 'involving', 'paternal', 'love', 'loyalty', 'and', 'betrayal', 'br', 'br', 'This', 'story', 'of', 'a', 'hit', 'man', 'Tom', 'Hanks', 'and', 'his', 'relationship', 'to', 'a', 'surrogate', 'father', 'figure', 'who', 'is', 'also', 'his', 'boss', 'an', 'elderly', 'Irish', 'mob', 'leader', 'Paul', 'Newman', 'seems', 'to', 'have', 'been', 'culled', 'from', 'innumerable', 'gangster', 'movies', 'of', 'years', 'past', 'The', 'father', 'son', 'motif', 'that', 'hangs', 'over', 'this', 'picture', 'is', 'so', 'heavy', 'handed', 'in', 'its', 'treatment', 'that', 'there', 'is', 'not', 'much', 'room', 'for', 'spontaneity', 'the', 'entire', 'enterprise', 'has', 'been', 'very', 'carefully', 'wrought', 'and', 'nearly', 'all', 'the', 'dialog', 'is', 'delivered', 'with', 'an', 'air', 'of', 'great', 'portent', 'this', 'is', 'obviously', 'a', 'gangster', 'film', 'hence', 'the', 'requisite', 'amount', 'of', 'violence', 'and', 'bloodshed', 'but', 'the', 'film', 'is', 'nearly', 'devoid', 'of', 'any', 'humor', 'to', 'speak', 'of', 'only', 'in', 'scenes', 'involving', 'a', 'young', 'boy', 'driving', 'a', 'getaway', 'car', 'in', 'a', 'cunningly', 'edited', 'montage', 'is', 'there', 'any', 'sense', 'of', 'lightheartedness', 'to', 'leaven', 'the', 'pervasive', 'sense', 'of', 'doom', 'br', 'br', 'That', 'being', 'said', 'I', 'have', 'nothing', 'but', 'the', 'highest', 'praise', 'for', 'the', 'stunning', 'look', 'of', 'this', 'film', 'indeed', 'it', 'is', 'not', 'an', 'overstatement', 'to', 'say', 'that', 'this', 'is', 'one', 'of', 'the', 'most', 'beautifully', 'photographed', 'and', 'designed', 'movies', 'I', 'have', 'ever', 'seen', 'Veteran', 'cameraman', 'Conrad', 'Hall', 'will', 'very', 'likely', 'win', 'another', 'Oscar', 'for', 'his', 'work', 'here', 'The', 'production', 's', 'sets', 'and', 'costumes', 'are', 'just', 'as', 'exemplary', 'in', 'fact', 'the', 'entire', 'film', 'is', 'a', 'technical', 'marvel', 'Mr', 'Mendes', 'continues', 'to', 'astonish', 'with', 'his', 'vivid', 'use', 'of', 'color', 'and', 'he', 'and', 'Mr', 'Hall', 'again', 'make', 'very', 'dramatic', 'use', 'of', 'red', 'blood', 'splattered', 'against', 'pale', 'colored', 'walls', 'all', 'the', 'more', 'effective', 'and', 'disconcerting', 'due', 'to', 'the', 'preponderance', 'of', 'blacks', 'blues', 'and', 'grays', 'that', 'dominate', 'the', 'movie', 's', 'color', 'scheme', 'br', 'br', 'If', 'I', 'have', 'failed', 'to', 'duly', 'note', 'the', 'acting', 'it', 'is', 'not', 'because', 'the', 'actors', 'do', 'not', 'purport', 'themselves', 'ably', 'everyone', 'in', 'the', 'film', 'is', 'top', 'notch', 'with', 'special', 'mention', 'going', 'to', 'the', 'two', 'malevolent', 'bad', 'guys', 'Daniel', 'Craig', 'is', 'the', 'classic', 'man', 'you', 'love', 'to', 'hate', 'the', 'spoiled', 'impulsive', 'son', 'of', 'Newman', 's', 'gangster', 'father', 'and', 'an', 'almost', 'unrecognizable', 'Jude', 'Law', 'as', 'an', 'especially', 'slimy', 'miscreant', 'who', 'goes', 'on', 'pursuit', 'of', 'Hanks', 'and', 'his', 'son', 'and', 'figures', 'very', 'importantly', 'in', 'the', 'film', 's', 'riveting', 'second', 'half', 'But', 'acting', 'in', 'a', 'movie', 'this', 'dazzling', 'is', 'bound', 'to', 'take', 'a', 'back', 'seat', 'to', 'the', 'photographic', 'fireworks', 'on', 'display', 'here', 'If', 'a', 'Rolls', 'Royce', 'was', 'a', 'movie', 'I', 've', 'no', 'doubt', 'it', 'would', 'look', 'like', 'ROAD', 'TO', 'PERDITION'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb1Bthmaj1Lr"
      },
      "source": [
        "TEXT.build_vocab(train_data,min_freq=100)\n",
        "LABEL.build_vocab(train_data,min_freq=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPcVJ-_F7v-n",
        "outputId": "900f46bb-66ba-45c7-ca3e-471e8b20f2ce"
      },
      "source": [
        "print(\"Unique tokens in TEXT vocabulary:\",len(TEXT.vocab))\n",
        "print(\"Unique tokens in LABEL vocabulary:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "print(LABEL.vocab.freqs)\n",
        "print(TEXT.unk_token)\n",
        "print(TEXT.pad_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 4539\n",
            "Unique tokens in LABEL vocabulary: 2\n",
            "[('the', 290180), ('and', 157009), ('a', 156519), ('of', 144139), ('to', 133996), ('is', 106267), ('br', 101870), ('in', 87845), ('I', 82128), ('it', 77806), ('that', 70381), ('s', 62818), ('this', 60804), ('was', 47841), ('The', 45081), ('as', 43579), ('movie', 43362), ('with', 42926), ('for', 42211), ('film', 39689)]\n",
            "Counter({'pos': 12500, 'neg': 12500})\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9PiSpoDmuy-"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=32, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRCXsKl5-u3e"
      },
      "source": [
        "Let's create our network;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBCxdM6joBbZ"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT.vocab), embedding_dim = 75,padding_idx = pad_idx)\n",
        "        self.layer1 = torch.nn.Linear(max_size*75, 10)\n",
        "        self.layer2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENDe2sd5rJn4",
        "outputId": "f0099e76-1e99-4a49-9354-2dd501039064"
      },
      "source": [
        "model = Network(pad_idx = TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(4539, 75, padding_idx=1)\n",
            "  (layer1): Linear(in_features=189375, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5B5TDLdqi93"
      },
      "source": [
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss() \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0Fw71NqNgD"
      },
      "source": [
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJUAGG5EDYJo"
      },
      "source": [
        "We can train our network now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpFvTmIyXGyR"
      },
      "source": [
        "def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "  correct = (torch.round(torch.sigmoid(predictions)) == labels).float() \n",
        "  accuracy = correct.sum() / len(correct)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVmtOVeVQJoM",
        "outputId": "e8a216f1-f3e6-48de-939e-3c0f016423aa"
      },
      "source": [
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "\n",
        "        optimizer.zero_grad()# Reset the gradients\n",
        "        loss.backward()# Backprop   \n",
        "        optimizer.step()# Optimize the weights\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "        if batch_no%60 == 0:\n",
        "          print(f'Epoch:  {epoch+1:2} | Batch No: {batch_no} | Loss: {loss.item():.3f} | Accuracy: {acc.item()*100:.2f}%')\n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss: {train_loss:.3f} ')\n",
        "    print('\\n') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1 | Batch No: 60 | Loss: 0.632 | Accuracy: 68.75%\n",
            "Epoch:   1 | Batch No: 120 | Loss: 0.872 | Accuracy: 56.25%\n",
            "Epoch:   1 | Batch No: 180 | Loss: 0.593 | Accuracy: 75.00%\n",
            "Epoch:   1 | Batch No: 240 | Loss: 0.542 | Accuracy: 78.12%\n",
            "Epoch:   1 | Batch No: 300 | Loss: 0.532 | Accuracy: 78.12%\n",
            "Epoch:   1 | Batch No: 360 | Loss: 0.397 | Accuracy: 87.50%\n",
            "Epoch:   1 | Batch No: 420 | Loss: 0.526 | Accuracy: 71.88%\n",
            "Epoch:   1 | Batch No: 480 | Loss: 0.573 | Accuracy: 75.00%\n",
            "Epoch:   1 | Batch No: 540 | Loss: 0.211 | Accuracy: 96.88%\n",
            "Epoch:   1 | Batch No: 600 | Loss: 0.440 | Accuracy: 87.50%\n",
            "Epoch:   1 | Batch No: 660 | Loss: 1.233 | Accuracy: 68.75%\n",
            "Epoch:   1 | Batch No: 720 | Loss: 0.511 | Accuracy: 78.12%\n",
            "Epoch:   1 | Batch No: 780 | Loss: 0.221 | Accuracy: 93.75%\n",
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time: 0m 58s\n",
            "\tAvarage Train Loss: 0.637 \n",
            "\n",
            "\n",
            "Epoch:   2 | Batch No: 60 | Loss: 0.655 | Accuracy: 93.75%\n",
            "Epoch:   2 | Batch No: 120 | Loss: 1.100 | Accuracy: 84.38%\n",
            "Epoch:   2 | Batch No: 180 | Loss: 1.860 | Accuracy: 87.50%\n",
            "Epoch:   2 | Batch No: 240 | Loss: 1.150 | Accuracy: 84.38%\n",
            "Epoch:   2 | Batch No: 300 | Loss: 2.688 | Accuracy: 84.38%\n",
            "Epoch:   2 | Batch No: 360 | Loss: 3.314 | Accuracy: 81.25%\n",
            "Epoch:   2 | Batch No: 420 | Loss: 0.101 | Accuracy: 93.75%\n",
            "Epoch:   2 | Batch No: 480 | Loss: 0.791 | Accuracy: 81.25%\n",
            "Epoch:   2 | Batch No: 540 | Loss: 4.721 | Accuracy: 78.12%\n",
            "Epoch:   2 | Batch No: 600 | Loss: 1.195 | Accuracy: 81.25%\n",
            "Epoch:   2 | Batch No: 660 | Loss: 1.390 | Accuracy: 81.25%\n",
            "Epoch:   2 | Batch No: 720 | Loss: 0.233 | Accuracy: 93.75%\n",
            "Epoch:   2 | Batch No: 780 | Loss: 0.536 | Accuracy: 87.50%\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time: 1m 1s\n",
            "\tAvarage Train Loss: 1.362 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCdD1EBy4W8V",
        "outputId": "2040be29-c57e-4508-ab06-16a935e1e597"
      },
      "source": [
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss = test_epoch_loss/len(test_iterator)\n",
        "test_acc = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.707 | | Test Acc: 64.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn3L5m05-j8y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}