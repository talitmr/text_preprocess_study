{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Negation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talitmr/text_preprocess_study/blob/main/Negation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAI8gIO4RBBC"
      },
      "source": [
        "# Negation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKm49Xzju28v"
      },
      "source": [
        "In this part, I tried to improve the preprocessing parts using stemming, lemmatization and both. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQNN8CFoRBBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c161c908-43a0-4f0c-f29d-4978509860ed"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__) #version of the pytorh\n",
        "import torch.nn.functional as F\n",
        "import torchtext.legacy as torchtext\n",
        "import random\n",
        "\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import pandas as pd\n",
        "import html\n",
        "import contractions\n",
        "from langdetect import detect\n",
        "from nltk import download\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.data import load\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "import re\n",
        "import nltk.data\n",
        "from nltk import pos_tag_sents\n",
        "# If you have Anaconda, you can install emoji using \n",
        "# \"conda install -c conda-forge emoji\" command. You can download autocorrect using pip\n",
        "# and \"target\" parameter: \"pip install autocorrect --target=<directory>\"\n",
        "from autocorrect import Speller\n",
        "from emoji import get_emoji_regexp\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import Levenshtein\n",
        "from snowballstemmer import TurkishStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from zeyrek import MorphAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.sentiment.util import mark_negation\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRW0mcRQn_PK"
      },
      "source": [
        "## Finding Max Length \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9juFZANBZEqX"
      },
      "source": [
        "First thing we need to do is finding the max length of the text in the dataset because our input size is constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz3mTFB3OOAo"
      },
      "source": [
        "## In this cell create \"tokenize_fn\" that takes text and returns tokens\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def tokenize_fn(text):\n",
        "\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "  return tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbaxYb7ivUoR"
      },
      "source": [
        "def stemming_fn(text):\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  sentence_stems = [stemmer.stem(token) for token in text]\n",
        "\n",
        "  return sentence_stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ladZlu-vbJ6"
      },
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "def get_lemmatizer_pos(pos):\n",
        "  pos_start = pos[0]# Takes the first letter to simplify the POS tag\n",
        "  if pos_start == \"J\":\n",
        "    return wn.ADJ\n",
        "  elif pos_start == \"V\":\n",
        "   return wn.VERB\n",
        "  elif pos_start == \"R\":\n",
        "   return wn.ADV\n",
        "  else:\n",
        "    return wn.NOUN \n",
        " \n",
        " \n",
        "def lemmatization_with_negation_fn(text):\n",
        "  sentence_tokens_pos = pos_tag(text)\n",
        "  sentence_lemmas = [lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1]))for token in sentence_tokens_pos]\n",
        "  sentence_lemmas_negated = mark_negation(sentence_lemmas)\n",
        "  return sentence_lemmas\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9Ni6oXTich-"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize=tokenize_fn, preprocessing= lemmatization_with_negation_fn, batch_first=True, lower = True) # preprocessing paraneters can be used to add aditional  preprocessing steps\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PEQVuI_m3uV"
      },
      "source": [
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8hrB9z4Hbak",
        "outputId": "e5dbb59a-204c-46ce-ffdd-75cdfd0d12c6"
      },
      "source": [
        "print(vars(train_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['i', 'f', 'you', 'think', 'sam', 'mendes', 'first', 'film', 'the', 'much', 'herald', 'american', 'beauty', 'be', 'a', 'movie', 'with', 'style', 'to', 'spare', 'wait', 'until', 'you', 'see', 'his', 'highly', 'anticipated', 'second', 'effort', 'the', 'unrelentingly', 'grim', '30', 's', 'gangster', 'melodrama', 'road', 'to', 'perdition', 'some', 'critic', 'have', 'hail', 'this', 'new', 'movie', 'a', 'a', 'worthy', 'successor', 'to', 'the', 'godfather', 'a', 'rash', 'judgment', 'make', 'by', 'several', 'reviewer', 'take', 'with', 'mr', 'mendes', 'extraordinary', 'technical', 'prowess', 'if', 'the', 'mechanic', 'of', 'movie', 'making', 'be', 'what', 'make', 'a', 'picture', 'great', 'then', 'yes', 'road', 'to', 'perdition', 'be', 'a', 'distant', 'cousin', 'to', 'the', 'godfather', 'in', 'term', 'of', 'what', 'it', 'achieve', 'in', 'cinematography', 'edit', 'music', 'scoring', 'and', 'sound', 'what', 'it', 'doesn', 't', 'have', 'be', 'a', 'resonance', 'that', 'all', 'great', 'story', 'and', 'some', 'very', 'rare', 'movie', 'have', 'that', 'stay', 'with', 'the', 'viewer', 'long', 'after', 'the', 'experience', 'of', 'reading', 'or', 'see', 'it', 'be', 'over', 'a', 'with', 'american', 'beauty', 'there', 'be', 'a', 'cold', 'distancing', 'feel', 'to', 'this', 'movie', 'despite', 'some', 'very', 'tense', 'scene', 'involve', 'paternal', 'love', 'loyalty', 'and', 'betrayal', 'br', 'br', 'this', 'story', 'of', 'a', 'hit', 'man', 'tom', 'hank', 'and', 'his', 'relationship', 'to', 'a', 'surrogate', 'father', 'figure', 'who', 'be', 'also', 'his', 'bos', 'an', 'elderly', 'irish', 'mob', 'leader', 'paul', 'newman', 'seem', 'to', 'have', 'be', 'cull', 'from', 'innumerable', 'gangster', 'movie', 'of', 'year', 'past', 'the', 'father', 'son', 'motif', 'that', 'hang', 'over', 'this', 'picture', 'be', 'so', 'heavy', 'hand', 'in', 'it', 'treatment', 'that', 'there', 'be', 'not', 'much', 'room', 'for', 'spontaneity', 'the', 'entire', 'enterprise', 'have', 'be', 'very', 'carefully', 'wrought', 'and', 'nearly', 'all', 'the', 'dialog', 'be', 'deliver', 'with', 'an', 'air', 'of', 'great', 'portent', 'this', 'be', 'obviously', 'a', 'gangster', 'film', 'hence', 'the', 'requisite', 'amount', 'of', 'violence', 'and', 'bloodshed', 'but', 'the', 'film', 'be', 'nearly', 'devoid', 'of', 'any', 'humor', 'to', 'speak', 'of', 'only', 'in', 'scene', 'involve', 'a', 'young', 'boy', 'drive', 'a', 'getaway', 'car', 'in', 'a', 'cunningly', 'edited', 'montage', 'be', 'there', 'any', 'sense', 'of', 'lightheartedness', 'to', 'leaven', 'the', 'pervasive', 'sense', 'of', 'doom', 'br', 'br', 'that', 'be', 'say', 'i', 'have', 'nothing', 'but', 'the', 'high', 'praise', 'for', 'the', 'stunning', 'look', 'of', 'this', 'film', 'indeed', 'it', 'be', 'not', 'an', 'overstatement', 'to', 'say', 'that', 'this', 'be', 'one', 'of', 'the', 'most', 'beautifully', 'photograph', 'and', 'design', 'movie', 'i', 'have', 'ever', 'see', 'veteran', 'cameraman', 'conrad', 'hall', 'will', 'very', 'likely', 'win', 'another', 'oscar', 'for', 'his', 'work', 'here', 'the', 'production', 's', 'set', 'and', 'costume', 'be', 'just', 'a', 'exemplary', 'in', 'fact', 'the', 'entire', 'film', 'be', 'a', 'technical', 'marvel', 'mr', 'mendes', 'continue', 'to', 'astonish', 'with', 'his', 'vivid', 'use', 'of', 'color', 'and', 'he', 'and', 'mr', 'hall', 'again', 'make', 'very', 'dramatic', 'use', 'of', 'red', 'blood', 'splatter', 'against', 'pale', 'color', 'wall', 'all', 'the', 'more', 'effective', 'and', 'disconcert', 'due', 'to', 'the', 'preponderance', 'of', 'black', 'blue', 'and', 'gray', 'that', 'dominate', 'the', 'movie', 's', 'color', 'scheme', 'br', 'br', 'if', 'i', 'have', 'fail', 'to', 'duly', 'note', 'the', 'act', 'it', 'be', 'not', 'because', 'the', 'actor', 'do', 'not', 'purport', 'themselves', 'ably', 'everyone', 'in', 'the', 'film', 'be', 'top', 'notch', 'with', 'special', 'mention', 'go', 'to', 'the', 'two', 'malevolent', 'bad', 'guy', 'daniel', 'craig', 'be', 'the', 'classic', 'man', 'you', 'love', 'to', 'hate', 'the', 'spoiled', 'impulsive', 'son', 'of', 'newman', 's', 'gangster', 'father', 'and', 'an', 'almost', 'unrecognizable', 'jude', 'law', 'a', 'an', 'especially', 'slimy', 'miscreant', 'who', 'go', 'on', 'pursuit', 'of', 'hank', 'and', 'his', 'son', 'and', 'figure', 'very', 'importantly', 'in', 'the', 'film', 's', 'rivet', 'second', 'half', 'but', 'act', 'in', 'a', 'movie', 'this', 'dazzling', 'be', 'bind', 'to', 'take', 'a', 'back', 'seat', 'to', 'the', 'photographic', 'firework', 'on', 'display', 'here', 'if', 'a', 'roll', 'royce', 'be', 'a', 'movie', 'i', 've', 'no', 'doubt', 'it', 'would', 'look', 'like', 'road', 'to', 'perdition'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac25bdxVXecP",
        "outputId": "ae4bca26-28e4-48df-cabd-53aa433c9fc6"
      },
      "source": [
        "max_size=0  ## this part of the code find maximum length of the network\n",
        "count=0\n",
        "sum= 0\n",
        "for i in  range(len(train_data)):\n",
        "  if max_size < len(train_data[i].text):\n",
        "    max_size =len(train_data[i].text)\n",
        "    print(max_size)\n",
        "  count +=1\n",
        "  sum +=len(train_data[0].text)\n",
        "print(\"avarage: \", sum/count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "559\n",
            "623\n",
            "1051\n",
            "1055\n",
            "1107\n",
            "1428\n",
            "1666\n",
            "1776\n",
            "1866\n",
            "1873\n",
            "2525\n",
            "avarage:  559.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG7I6kZ7oHKX"
      },
      "source": [
        "## Training Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy6YWYP4oN58"
      },
      "source": [
        "We just have found the max length corpus. Now let's create our train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_wJl9cwmuR0",
        "outputId": "28645c38-e376-4a81-9444-5fd0b2865420"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize=tokenize_fn, preprocessing= lemmatization_with_negation_fn, batch_first=True,fix_length=max_size, lower=True ) # preprocessing parameters can be used to add aditional  preprocessing steps\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL) \n",
        "print(\"train length is: \",len(train_data))\n",
        "print(\"test length is: \",len(test_data))\n",
        "print(vars(train_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train length is:  25000\n",
            "test length is:  25000\n",
            "{'text': ['i', 'f', 'you', 'think', 'sam', 'mendes', 'first', 'film', 'the', 'much', 'herald', 'american', 'beauty', 'be', 'a', 'movie', 'with', 'style', 'to', 'spare', 'wait', 'until', 'you', 'see', 'his', 'highly', 'anticipated', 'second', 'effort', 'the', 'unrelentingly', 'grim', '30', 's', 'gangster', 'melodrama', 'road', 'to', 'perdition', 'some', 'critic', 'have', 'hail', 'this', 'new', 'movie', 'a', 'a', 'worthy', 'successor', 'to', 'the', 'godfather', 'a', 'rash', 'judgment', 'make', 'by', 'several', 'reviewer', 'take', 'with', 'mr', 'mendes', 'extraordinary', 'technical', 'prowess', 'if', 'the', 'mechanic', 'of', 'movie', 'making', 'be', 'what', 'make', 'a', 'picture', 'great', 'then', 'yes', 'road', 'to', 'perdition', 'be', 'a', 'distant', 'cousin', 'to', 'the', 'godfather', 'in', 'term', 'of', 'what', 'it', 'achieve', 'in', 'cinematography', 'edit', 'music', 'scoring', 'and', 'sound', 'what', 'it', 'doesn', 't', 'have', 'be', 'a', 'resonance', 'that', 'all', 'great', 'story', 'and', 'some', 'very', 'rare', 'movie', 'have', 'that', 'stay', 'with', 'the', 'viewer', 'long', 'after', 'the', 'experience', 'of', 'reading', 'or', 'see', 'it', 'be', 'over', 'a', 'with', 'american', 'beauty', 'there', 'be', 'a', 'cold', 'distancing', 'feel', 'to', 'this', 'movie', 'despite', 'some', 'very', 'tense', 'scene', 'involve', 'paternal', 'love', 'loyalty', 'and', 'betrayal', 'br', 'br', 'this', 'story', 'of', 'a', 'hit', 'man', 'tom', 'hank', 'and', 'his', 'relationship', 'to', 'a', 'surrogate', 'father', 'figure', 'who', 'be', 'also', 'his', 'bos', 'an', 'elderly', 'irish', 'mob', 'leader', 'paul', 'newman', 'seem', 'to', 'have', 'be', 'cull', 'from', 'innumerable', 'gangster', 'movie', 'of', 'year', 'past', 'the', 'father', 'son', 'motif', 'that', 'hang', 'over', 'this', 'picture', 'be', 'so', 'heavy', 'hand', 'in', 'it', 'treatment', 'that', 'there', 'be', 'not', 'much', 'room', 'for', 'spontaneity', 'the', 'entire', 'enterprise', 'have', 'be', 'very', 'carefully', 'wrought', 'and', 'nearly', 'all', 'the', 'dialog', 'be', 'deliver', 'with', 'an', 'air', 'of', 'great', 'portent', 'this', 'be', 'obviously', 'a', 'gangster', 'film', 'hence', 'the', 'requisite', 'amount', 'of', 'violence', 'and', 'bloodshed', 'but', 'the', 'film', 'be', 'nearly', 'devoid', 'of', 'any', 'humor', 'to', 'speak', 'of', 'only', 'in', 'scene', 'involve', 'a', 'young', 'boy', 'drive', 'a', 'getaway', 'car', 'in', 'a', 'cunningly', 'edited', 'montage', 'be', 'there', 'any', 'sense', 'of', 'lightheartedness', 'to', 'leaven', 'the', 'pervasive', 'sense', 'of', 'doom', 'br', 'br', 'that', 'be', 'say', 'i', 'have', 'nothing', 'but', 'the', 'high', 'praise', 'for', 'the', 'stunning', 'look', 'of', 'this', 'film', 'indeed', 'it', 'be', 'not', 'an', 'overstatement', 'to', 'say', 'that', 'this', 'be', 'one', 'of', 'the', 'most', 'beautifully', 'photograph', 'and', 'design', 'movie', 'i', 'have', 'ever', 'see', 'veteran', 'cameraman', 'conrad', 'hall', 'will', 'very', 'likely', 'win', 'another', 'oscar', 'for', 'his', 'work', 'here', 'the', 'production', 's', 'set', 'and', 'costume', 'be', 'just', 'a', 'exemplary', 'in', 'fact', 'the', 'entire', 'film', 'be', 'a', 'technical', 'marvel', 'mr', 'mendes', 'continue', 'to', 'astonish', 'with', 'his', 'vivid', 'use', 'of', 'color', 'and', 'he', 'and', 'mr', 'hall', 'again', 'make', 'very', 'dramatic', 'use', 'of', 'red', 'blood', 'splatter', 'against', 'pale', 'color', 'wall', 'all', 'the', 'more', 'effective', 'and', 'disconcert', 'due', 'to', 'the', 'preponderance', 'of', 'black', 'blue', 'and', 'gray', 'that', 'dominate', 'the', 'movie', 's', 'color', 'scheme', 'br', 'br', 'if', 'i', 'have', 'fail', 'to', 'duly', 'note', 'the', 'act', 'it', 'be', 'not', 'because', 'the', 'actor', 'do', 'not', 'purport', 'themselves', 'ably', 'everyone', 'in', 'the', 'film', 'be', 'top', 'notch', 'with', 'special', 'mention', 'go', 'to', 'the', 'two', 'malevolent', 'bad', 'guy', 'daniel', 'craig', 'be', 'the', 'classic', 'man', 'you', 'love', 'to', 'hate', 'the', 'spoiled', 'impulsive', 'son', 'of', 'newman', 's', 'gangster', 'father', 'and', 'an', 'almost', 'unrecognizable', 'jude', 'law', 'a', 'an', 'especially', 'slimy', 'miscreant', 'who', 'go', 'on', 'pursuit', 'of', 'hank', 'and', 'his', 'son', 'and', 'figure', 'very', 'importantly', 'in', 'the', 'film', 's', 'rivet', 'second', 'half', 'but', 'act', 'in', 'a', 'movie', 'this', 'dazzling', 'be', 'bind', 'to', 'take', 'a', 'back', 'seat', 'to', 'the', 'photographic', 'firework', 'on', 'display', 'here', 'if', 'a', 'roll', 'royce', 'be', 'a', 'movie', 'i', 've', 'no', 'doubt', 'it', 'would', 'look', 'like', 'road', 'to', 'perdition'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb1Bthmaj1Lr"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOLkG_dgjxH4",
        "outputId": "f7097794-d929-46fb-fcec-889375373f21"
      },
      "source": [
        "print(\"Unique tokens in TEXT vocabulary:\",len(TEXT.vocab))\n",
        "print(\"Unique tokens in LABEL vocabulary:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "print(LABEL.vocab.freqs)\n",
        "print(TEXT.unk_token)\n",
        "print(TEXT.pad_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 64737\n",
            "Unique tokens in LABEL vocabulary: 2\n",
            "[('the', 336749), ('be', 241407), ('a', 203889), ('and', 164140), ('of', 145864), ('to', 135724), ('it', 104648), ('br', 101871), ('in', 94015), ('i', 87690), ('this', 76007), ('that', 73286), ('s', 63633), ('have', 58357), ('movie', 51715), ('film', 47940), ('for', 44345), ('with', 44130), ('but', 42623), ('t', 34390)]\n",
            "Counter({'pos': 12500, 'neg': 12500})\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9PiSpoDmuy-"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "##Create a train and test iterators using Bucket iterator method with batch size 32\n",
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train_data, test_data), batch_size=32, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRCXsKl5-u3e"
      },
      "source": [
        "Let's create our network;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBCxdM6joBbZ"
      },
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self,pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings = len(TEXT.vocab), embedding_dim = 100,padding_idx = pad_idx)\n",
        "        self.layer1 = torch.nn.Linear(max_size*100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.size(0),-1)\n",
        "        x = self.layer1(x)\n",
        "        return x       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENDe2sd5rJn4",
        "outputId": "58dfdaee-5177-4641-981a-a038a3a42f65"
      },
      "source": [
        "model = Network(pad_idx = TEXT.vocab.stoi[TEXT.pad_token])\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Embedding(64737, 100, padding_idx=1)\n",
            "  (layer1): Linear(in_features=252500, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5B5TDLdqi93"
      },
      "source": [
        "# Choose a Loss function from torch.nn according to your network\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss() \n",
        "\n",
        "#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\n",
        "optimizer = torch.optim.Adam(params= model.parameters(),lr= 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0Fw71NqNgD"
      },
      "source": [
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJUAGG5EDYJo"
      },
      "source": [
        "We can train our network now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpFvTmIyXGyR"
      },
      "source": [
        "def accuracy_fn(predictions, labels):  ## create a accuraccy function for further use\n",
        "  correct = (torch.round(torch.sigmoid(predictions)) == labels).float() \n",
        "  accuracy = correct.sum() / len(correct)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVmtOVeVQJoM",
        "outputId": "40802c9c-eb54-41bd-d4dd-a9137e09576c"
      },
      "source": [
        "import time\n",
        "# Training loop\n",
        "N_EPOCHS = 2\n",
        "\n",
        "tr_loss = []\n",
        "model.train()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    # Calculate training time\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    \n",
        "    batch_no = 0\n",
        "    for batch in train_iterator:\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = loss_fn(predictions, batch.label.squeeze(0))\n",
        "\n",
        "        optimizer.zero_grad()# Reset the gradients\n",
        "        loss.backward()# Backprop   \n",
        "        optimizer.step()# Optimize the weights\n",
        "        ##################################\n",
        "\n",
        "        # Record accuracy and loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        acc =   accuracy_fn(predictions,batch.label.squeeze(0)) \n",
        "        epoch_acc +=acc.item()\n",
        "\n",
        "        batch_no = batch_no +1\n",
        "        \n",
        "        if batch_no%60 == 0:\n",
        "          print(f'Epoch:  {epoch+1:2} | Batch No: {batch_no} | Loss: {loss.item():.3f} | Accuracy: {acc.item()*100:.2f}%')\n",
        "\n",
        "    \n",
        "    train_loss = epoch_loss / len(train_iterator)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    \n",
        "    print('\\n')    \n",
        "    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
        "    print(f'\\tAvarage Train Loss: {train_loss:.3f} ')\n",
        "    print('\\n') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1 | Batch No: 60 | Loss: 0.971 | Accuracy: 50.00%\n",
            "Epoch:   1 | Batch No: 120 | Loss: 0.905 | Accuracy: 46.88%\n",
            "Epoch:   1 | Batch No: 180 | Loss: 0.813 | Accuracy: 62.50%\n",
            "Epoch:   1 | Batch No: 240 | Loss: 0.548 | Accuracy: 71.88%\n",
            "Epoch:   1 | Batch No: 300 | Loss: 1.082 | Accuracy: 59.38%\n",
            "Epoch:   1 | Batch No: 360 | Loss: 1.296 | Accuracy: 62.50%\n",
            "Epoch:   1 | Batch No: 420 | Loss: 0.708 | Accuracy: 68.75%\n",
            "Epoch:   1 | Batch No: 480 | Loss: 0.741 | Accuracy: 62.50%\n",
            "Epoch:   1 | Batch No: 540 | Loss: 0.856 | Accuracy: 62.50%\n",
            "Epoch:   1 | Batch No: 600 | Loss: 0.514 | Accuracy: 78.12%\n",
            "Epoch:   1 | Batch No: 660 | Loss: 0.860 | Accuracy: 68.75%\n",
            "Epoch:   1 | Batch No: 720 | Loss: 1.429 | Accuracy: 59.38%\n",
            "Epoch:   1 | Batch No: 780 | Loss: 1.327 | Accuracy: 56.25%\n",
            "\n",
            "\n",
            "Epoch:  1 | Epoch Time: 1m 27s\n",
            "\tAvarage Train Loss: 1.031 \n",
            "\n",
            "\n",
            "Epoch:   2 | Batch No: 60 | Loss: 0.434 | Accuracy: 93.75%\n",
            "Epoch:   2 | Batch No: 120 | Loss: 0.288 | Accuracy: 87.50%\n",
            "Epoch:   2 | Batch No: 180 | Loss: 0.282 | Accuracy: 90.62%\n",
            "Epoch:   2 | Batch No: 240 | Loss: 0.198 | Accuracy: 90.62%\n",
            "Epoch:   2 | Batch No: 300 | Loss: 0.166 | Accuracy: 96.88%\n",
            "Epoch:   2 | Batch No: 360 | Loss: 0.283 | Accuracy: 90.62%\n",
            "Epoch:   2 | Batch No: 420 | Loss: 0.177 | Accuracy: 93.75%\n",
            "Epoch:   2 | Batch No: 480 | Loss: 0.387 | Accuracy: 87.50%\n",
            "Epoch:   2 | Batch No: 540 | Loss: 0.343 | Accuracy: 87.50%\n",
            "Epoch:   2 | Batch No: 600 | Loss: 0.370 | Accuracy: 87.50%\n",
            "Epoch:   2 | Batch No: 660 | Loss: 0.227 | Accuracy: 93.75%\n",
            "Epoch:   2 | Batch No: 720 | Loss: 0.311 | Accuracy: 84.38%\n",
            "Epoch:   2 | Batch No: 780 | Loss: 0.138 | Accuracy: 90.62%\n",
            "\n",
            "\n",
            "Epoch:  2 | Epoch Time: 1m 34s\n",
            "\tAvarage Train Loss: 0.314 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCdD1EBy4W8V",
        "outputId": "42d4846f-e0d8-40fb-91c1-67e9b466e492"
      },
      "source": [
        "test_epoch_loss = 0\n",
        "test_epoch_acc = 0\n",
        "\n",
        "# Turm on evalutaion mode\n",
        "model.eval()\n",
        "\n",
        "# No need to backprop in eval\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        test_predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        test_loss = loss_fn(test_predictions, batch.label)\n",
        "\n",
        "        test_epoch_loss += test_loss.item()\n",
        "        \n",
        "        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n",
        "        test_epoch_acc +=acc.item()\n",
        "\n",
        "test_loss = test_epoch_loss/len(test_iterator)\n",
        "test_acc = test_epoch_acc  / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.035 | | Test Acc: 69.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-utU5srgO-Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}